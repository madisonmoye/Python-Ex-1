{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Python Ex 3: Using LASSO Regression to Model Fat Sales Data","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"XzCvxddXAsoQ","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import pandas\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LassoLarsCV\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IxlYOGB6BHsp","colab_type":"code","outputId":"4ca4a466-0852-4d69-e3f1-55bdd7f7933e","executionInfo":{"status":"ok","timestamp":1574908287660,"user_tz":420,"elapsed":2244,"user":{"displayName":"Madison Moye","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCbK9EQQQiDjMLG5iuvLnuGujaQR4mOw2TbVbWR2w=s64","userId":"01451677567117313270"}},"colab":{"base_uri":"https://localhost:8080/","height":270}},"source":["DATA_URL = 'http://128.138.93.164/aprd6342/data/finalmaster-ratios.csv'\n","alldata = pd.read_csv(DATA_URL)\n","alldata.head()"],"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th># Purchases</th>\n","      <th>B01001001</th>\n","      <th>B01001002</th>\n","      <th>B01001003</th>\n","      <th>B01001004</th>\n","      <th>B01001005</th>\n","      <th>B01001006</th>\n","      <th>B01001007</th>\n","      <th>B01001008</th>\n","      <th>B01001009</th>\n","      <th>B01001010</th>\n","      <th>B01001011</th>\n","      <th>B01001012</th>\n","      <th>B01001013</th>\n","      <th>B01001014</th>\n","      <th>B01001015</th>\n","      <th>B01001016</th>\n","      <th>B01001017</th>\n","      <th>B01001018</th>\n","      <th>B01001019</th>\n","      <th>B01001020</th>\n","      <th>B01001021</th>\n","      <th>B01001022</th>\n","      <th>B01001023</th>\n","      <th>B01001024</th>\n","      <th>B01001025</th>\n","      <th>B01001026</th>\n","      <th>B01001027</th>\n","      <th>B01001028</th>\n","      <th>B01001029</th>\n","      <th>B01001030</th>\n","      <th>B01001031</th>\n","      <th>B01001032</th>\n","      <th>B01001033</th>\n","      <th>B01001034</th>\n","      <th>B01001035</th>\n","      <th>B01001036</th>\n","      <th>B01001037</th>\n","      <th>B01001038</th>\n","      <th>B01001039</th>\n","      <th>...</th>\n","      <th>B15002013</th>\n","      <th>B15002014</th>\n","      <th>B15002015</th>\n","      <th>B15002016</th>\n","      <th>B15002017</th>\n","      <th>B15002018</th>\n","      <th>B15002019</th>\n","      <th>B15002020</th>\n","      <th>B15002021</th>\n","      <th>B15002022</th>\n","      <th>B15002023</th>\n","      <th>B15002024</th>\n","      <th>B15002025</th>\n","      <th>B15002026</th>\n","      <th>B15002027</th>\n","      <th>B15002028</th>\n","      <th>B15002029</th>\n","      <th>B15002030</th>\n","      <th>B15002031</th>\n","      <th>B15002032</th>\n","      <th>B15002033</th>\n","      <th>B15002034</th>\n","      <th>B15002035</th>\n","      <th>B19001001</th>\n","      <th>B19001002</th>\n","      <th>B19001003</th>\n","      <th>B19001004</th>\n","      <th>B19001005</th>\n","      <th>B19001006</th>\n","      <th>B19001007</th>\n","      <th>B19001008</th>\n","      <th>B19001009</th>\n","      <th>B19001010</th>\n","      <th>B19001011</th>\n","      <th>B19001012</th>\n","      <th>B19001013</th>\n","      <th>B19001014</th>\n","      <th>B19001015</th>\n","      <th>B19001016</th>\n","      <th>B19001017</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>22</td>\n","      <td>206252</td>\n","      <td>469.226965</td>\n","      <td>31.432422</td>\n","      <td>35.219052</td>\n","      <td>33.628765</td>\n","      <td>20.121017</td>\n","      <td>12.610787</td>\n","      <td>6.734480</td>\n","      <td>6.225394</td>\n","      <td>19.432539</td>\n","      <td>28.101546</td>\n","      <td>28.421543</td>\n","      <td>26.390047</td>\n","      <td>31.989993</td>\n","      <td>31.359696</td>\n","      <td>32.116052</td>\n","      <td>32.213021</td>\n","      <td>12.184124</td>\n","      <td>18.361034</td>\n","      <td>9.454454</td>\n","      <td>15.175610</td>\n","      <td>16.281054</td>\n","      <td>11.025348</td>\n","      <td>6.230243</td>\n","      <td>4.518744</td>\n","      <td>530.773035</td>\n","      <td>31.999690</td>\n","      <td>34.322091</td>\n","      <td>32.649380</td>\n","      <td>20.101623</td>\n","      <td>12.513818</td>\n","      <td>8.072649</td>\n","      <td>6.021760</td>\n","      <td>22.923414</td>\n","      <td>31.335454</td>\n","      <td>31.558482</td>\n","      <td>31.063941</td>\n","      <td>36.082074</td>\n","      <td>34.845723</td>\n","      <td>...</td>\n","      <td>64.610300</td>\n","      <td>31.449746</td>\n","      <td>58.735313</td>\n","      <td>20.071053</td>\n","      <td>6.726751</td>\n","      <td>5.882267</td>\n","      <td>543.803963</td>\n","      <td>6.974272</td>\n","      <td>2.504332</td>\n","      <td>5.904107</td>\n","      <td>11.917415</td>\n","      <td>10.767170</td>\n","      <td>18.141844</td>\n","      <td>19.779852</td>\n","      <td>10.956451</td>\n","      <td>181.418442</td>\n","      <td>26.717724</td>\n","      <td>85.271036</td>\n","      <td>54.243532</td>\n","      <td>72.647457</td>\n","      <td>30.816383</td>\n","      <td>2.831933</td>\n","      <td>2.912014</td>\n","      <td>1000</td>\n","      <td>105.667996</td>\n","      <td>82.298375</td>\n","      <td>68.141163</td>\n","      <td>67.336195</td>\n","      <td>63.566902</td>\n","      <td>59.439845</td>\n","      <td>49.409690</td>\n","      <td>53.306757</td>\n","      <td>42.318307</td>\n","      <td>83.167229</td>\n","      <td>89.249208</td>\n","      <td>102.141470</td>\n","      <td>52.872330</td>\n","      <td>36.440765</td>\n","      <td>23.446284</td>\n","      <td>21.197485</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7</td>\n","      <td>61399</td>\n","      <td>486.538869</td>\n","      <td>22.899396</td>\n","      <td>21.531295</td>\n","      <td>27.036271</td>\n","      <td>16.808091</td>\n","      <td>28.355511</td>\n","      <td>18.192479</td>\n","      <td>13.534422</td>\n","      <td>21.466148</td>\n","      <td>24.886399</td>\n","      <td>23.534585</td>\n","      <td>21.319565</td>\n","      <td>27.101419</td>\n","      <td>30.961416</td>\n","      <td>37.117868</td>\n","      <td>36.466392</td>\n","      <td>12.557208</td>\n","      <td>20.554081</td>\n","      <td>12.182609</td>\n","      <td>15.651721</td>\n","      <td>20.668089</td>\n","      <td>15.961172</td>\n","      <td>10.423623</td>\n","      <td>7.329110</td>\n","      <td>513.461131</td>\n","      <td>18.974250</td>\n","      <td>23.404290</td>\n","      <td>23.892897</td>\n","      <td>17.036108</td>\n","      <td>35.310021</td>\n","      <td>18.534504</td>\n","      <td>17.101256</td>\n","      <td>22.785387</td>\n","      <td>22.150198</td>\n","      <td>22.622518</td>\n","      <td>21.303279</td>\n","      <td>26.971123</td>\n","      <td>32.329517</td>\n","      <td>...</td>\n","      <td>56.929829</td>\n","      <td>46.381727</td>\n","      <td>65.707446</td>\n","      <td>35.509451</td>\n","      <td>16.782205</td>\n","      <td>9.201536</td>\n","      <td>515.086529</td>\n","      <td>3.017306</td>\n","      <td>1.047329</td>\n","      <td>1.371503</td>\n","      <td>6.358785</td>\n","      <td>4.937410</td>\n","      <td>8.303825</td>\n","      <td>9.700264</td>\n","      <td>7.555733</td>\n","      <td>174.155902</td>\n","      <td>25.834123</td>\n","      <td>60.146626</td>\n","      <td>62.440776</td>\n","      <td>76.604658</td>\n","      <td>55.383771</td>\n","      <td>8.977108</td>\n","      <td>9.251409</td>\n","      <td>1000</td>\n","      <td>71.289558</td>\n","      <td>59.062447</td>\n","      <td>54.704688</td>\n","      <td>60.966323</td>\n","      <td>53.012354</td>\n","      <td>60.881706</td>\n","      <td>59.231680</td>\n","      <td>50.093078</td>\n","      <td>40.700626</td>\n","      <td>92.612963</td>\n","      <td>117.363344</td>\n","      <td>113.344051</td>\n","      <td>75.774243</td>\n","      <td>33.000508</td>\n","      <td>33.169741</td>\n","      <td>24.792689</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>73170</td>\n","      <td>489.859232</td>\n","      <td>28.905289</td>\n","      <td>36.271696</td>\n","      <td>28.235616</td>\n","      <td>21.566216</td>\n","      <td>12.218122</td>\n","      <td>7.243406</td>\n","      <td>7.380074</td>\n","      <td>16.933169</td>\n","      <td>24.914582</td>\n","      <td>26.896269</td>\n","      <td>31.802651</td>\n","      <td>30.531639</td>\n","      <td>36.258029</td>\n","      <td>35.998360</td>\n","      <td>33.429001</td>\n","      <td>13.625803</td>\n","      <td>19.406861</td>\n","      <td>12.245456</td>\n","      <td>14.664480</td>\n","      <td>21.169878</td>\n","      <td>15.293153</td>\n","      <td>8.610086</td>\n","      <td>6.259396</td>\n","      <td>510.140768</td>\n","      <td>26.171928</td>\n","      <td>30.681973</td>\n","      <td>31.925653</td>\n","      <td>19.789531</td>\n","      <td>10.072434</td>\n","      <td>5.056717</td>\n","      <td>6.218396</td>\n","      <td>15.757824</td>\n","      <td>24.449911</td>\n","      <td>26.595599</td>\n","      <td>27.210605</td>\n","      <td>37.556376</td>\n","      <td>37.050704</td>\n","      <td>...</td>\n","      <td>54.602613</td>\n","      <td>40.613027</td>\n","      <td>43.363788</td>\n","      <td>12.280185</td>\n","      <td>5.796247</td>\n","      <td>3.438452</td>\n","      <td>523.980745</td>\n","      <td>5.422930</td>\n","      <td>4.224384</td>\n","      <td>11.828274</td>\n","      <td>18.331860</td>\n","      <td>15.089891</td>\n","      <td>21.731015</td>\n","      <td>18.685529</td>\n","      <td>7.014441</td>\n","      <td>155.241183</td>\n","      <td>45.466156</td>\n","      <td>71.185775</td>\n","      <td>65.802142</td>\n","      <td>56.272718</td>\n","      <td>24.580018</td>\n","      <td>1.689753</td>\n","      <td>1.414677</td>\n","      <td>1000</td>\n","      <td>102.538696</td>\n","      <td>82.960331</td>\n","      <td>74.828305</td>\n","      <td>79.133495</td>\n","      <td>66.081252</td>\n","      <td>78.245122</td>\n","      <td>63.996993</td>\n","      <td>47.322923</td>\n","      <td>42.505211</td>\n","      <td>70.420610</td>\n","      <td>90.033143</td>\n","      <td>98.677692</td>\n","      <td>54.703249</td>\n","      <td>20.125056</td>\n","      <td>11.890525</td>\n","      <td>16.537397</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>94</td>\n","      <td>251724</td>\n","      <td>505.585483</td>\n","      <td>32.054949</td>\n","      <td>31.757004</td>\n","      <td>28.102207</td>\n","      <td>18.651380</td>\n","      <td>12.080692</td>\n","      <td>7.035483</td>\n","      <td>7.686991</td>\n","      <td>25.790151</td>\n","      <td>42.129475</td>\n","      <td>35.824951</td>\n","      <td>32.058922</td>\n","      <td>27.677138</td>\n","      <td>33.842621</td>\n","      <td>38.176733</td>\n","      <td>32.722347</td>\n","      <td>12.493842</td>\n","      <td>16.394940</td>\n","      <td>11.504664</td>\n","      <td>15.914255</td>\n","      <td>16.394940</td>\n","      <td>13.196994</td>\n","      <td>8.648361</td>\n","      <td>5.446441</td>\n","      <td>494.414517</td>\n","      <td>33.123580</td>\n","      <td>28.082344</td>\n","      <td>30.171934</td>\n","      <td>16.863708</td>\n","      <td>9.280005</td>\n","      <td>5.390825</td>\n","      <td>5.609318</td>\n","      <td>19.453846</td>\n","      <td>35.614403</td>\n","      <td>32.082757</td>\n","      <td>28.809331</td>\n","      <td>27.911522</td>\n","      <td>32.690566</td>\n","      <td>...</td>\n","      <td>88.227492</td>\n","      <td>44.076261</td>\n","      <td>87.939148</td>\n","      <td>44.404973</td>\n","      <td>9.671057</td>\n","      <td>7.283569</td>\n","      <td>502.912274</td>\n","      <td>4.509700</td>\n","      <td>0.980370</td>\n","      <td>3.552398</td>\n","      <td>5.986021</td>\n","      <td>7.398907</td>\n","      <td>9.740260</td>\n","      <td>10.605292</td>\n","      <td>7.485410</td>\n","      <td>141.242417</td>\n","      <td>43.078591</td>\n","      <td>84.479020</td>\n","      <td>52.069156</td>\n","      <td>89.836451</td>\n","      <td>33.932320</td>\n","      <td>4.129086</td>\n","      <td>3.886877</td>\n","      <td>1000</td>\n","      <td>61.632139</td>\n","      <td>46.526521</td>\n","      <td>48.437595</td>\n","      <td>54.221644</td>\n","      <td>51.680322</td>\n","      <td>60.066684</td>\n","      <td>54.790900</td>\n","      <td>48.681562</td>\n","      <td>43.873381</td>\n","      <td>84.717507</td>\n","      <td>112.204444</td>\n","      <td>127.137252</td>\n","      <td>83.019904</td>\n","      <td>43.731067</td>\n","      <td>38.851729</td>\n","      <td>40.427349</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>37382</td>\n","      <td>495.586111</td>\n","      <td>25.413301</td>\n","      <td>29.318924</td>\n","      <td>26.162324</td>\n","      <td>19.260607</td>\n","      <td>12.893906</td>\n","      <td>6.580707</td>\n","      <td>7.062222</td>\n","      <td>17.334546</td>\n","      <td>32.930287</td>\n","      <td>28.302392</td>\n","      <td>28.569900</td>\n","      <td>26.804344</td>\n","      <td>30.549462</td>\n","      <td>36.595153</td>\n","      <td>42.373335</td>\n","      <td>16.398267</td>\n","      <td>22.871970</td>\n","      <td>17.174041</td>\n","      <td>15.221229</td>\n","      <td>23.433738</td>\n","      <td>14.391953</td>\n","      <td>7.383233</td>\n","      <td>8.560270</td>\n","      <td>504.413889</td>\n","      <td>26.563587</td>\n","      <td>30.255203</td>\n","      <td>24.798031</td>\n","      <td>16.237761</td>\n","      <td>11.101600</td>\n","      <td>4.788401</td>\n","      <td>5.189663</td>\n","      <td>17.842812</td>\n","      <td>30.014445</td>\n","      <td>27.767375</td>\n","      <td>30.763469</td>\n","      <td>25.199294</td>\n","      <td>29.613183</td>\n","      <td>...</td>\n","      <td>102.957039</td>\n","      <td>36.711921</td>\n","      <td>70.039055</td>\n","      <td>33.587502</td>\n","      <td>5.021387</td>\n","      <td>5.244560</td>\n","      <td>511.177236</td>\n","      <td>2.045750</td>\n","      <td>3.236005</td>\n","      <td>1.525014</td>\n","      <td>7.476288</td>\n","      <td>4.314674</td>\n","      <td>8.554956</td>\n","      <td>13.204389</td>\n","      <td>7.773852</td>\n","      <td>130.890831</td>\n","      <td>53.784638</td>\n","      <td>99.311884</td>\n","      <td>57.095034</td>\n","      <td>76.027525</td>\n","      <td>38.422912</td>\n","      <td>4.351869</td>\n","      <td>3.161614</td>\n","      <td>1000</td>\n","      <td>51.125525</td>\n","      <td>58.438255</td>\n","      <td>68.930434</td>\n","      <td>74.717029</td>\n","      <td>63.970495</td>\n","      <td>59.710034</td>\n","      <td>58.883378</td>\n","      <td>51.761414</td>\n","      <td>47.310187</td>\n","      <td>81.902582</td>\n","      <td>93.793717</td>\n","      <td>130.103014</td>\n","      <td>71.982704</td>\n","      <td>36.118530</td>\n","      <td>31.603714</td>\n","      <td>19.648989</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 190 columns</p>\n","</div>"],"text/plain":["   # Purchases  B01001001   B01001002  ...  B19001015  B19001016  B19001017\n","0           22     206252  469.226965  ...  36.440765  23.446284  21.197485\n","1            7      61399  486.538869  ...  33.000508  33.169741  24.792689\n","2            3      73170  489.859232  ...  20.125056  11.890525  16.537397\n","3           94     251724  505.585483  ...  43.731067  38.851729  40.427349\n","4            0      37382  495.586111  ...  36.118530  31.603714  19.648989\n","\n","[5 rows x 190 columns]"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"markdown","metadata":{"id":"vk61sRpQCyzS","colab_type":"text"},"source":["First, I want you to create a list of all the predictors you're going to feed into the LassoLarsCV model. Assuming you've loaded your data into a pandas dataframe and named that dataframe alldata, you can get a list of your variables easily:"]},{"cell_type":"code","metadata":{"id":"e0cOO6YnCP0Q","colab_type":"code","colab":{}},"source":["allvariablenames = list(alldata.columns.values)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UKBtHh7KDIIP","colab_type":"text"},"source":["The first variable in the list is the outcome variable, and the next 7 are repetitive of other variables, so we need to exclude them. Go ahead and remove those items from the list."]},{"cell_type":"code","metadata":{"id":"1D3LhhkMC8Ea","colab_type":"code","colab":{}},"source":["listofallpredictors = allvariablenames[8:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qCjc6yNFDPZA","colab_type":"code","colab":{}},"source":["#target variable \n","#load predictors into dataframe\n","predictors = alldata[listofallpredictors]  \n","\n","#load target into dataframe\n","target = alldata['# Purchases']   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Yxf3GHxPK8X","colab_type":"code","colab":{}},"source":["# split data into train and test sets, with 30% retained for test\n","\n","pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size=.3, random_state=123)    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ydegtQllK7Em","colab_type":"text"},"source":["Use LassoLarsCV and feed the following parameters in:\n","\n","cv = 10 (this performs a 10-fold cross validation, to make sure our results aren't do to random ordering of the data)\n","precompute=False (not necessary)\n","name this model 'model'."]},{"cell_type":"code","metadata":{"id":"spPKKPCsKwO6","colab_type":"code","colab":{}},"source":["model = LassoLarsCV(precompute=False, cv=10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1FoKciGaQnFH","colab_type":"text"},"source":["Next, we need to fit our newly created model with your training data. "]},{"cell_type":"code","metadata":{"id":"jSgndbR2Q085","colab_type":"code","outputId":"bdb0e3b9-f672-44bf-84d8-ea049255801d","executionInfo":{"status":"ok","timestamp":1574908301769,"user_tz":420,"elapsed":961,"user":{"displayName":"Madison Moye","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCbK9EQQQiDjMLG5iuvLnuGujaQR4mOw2TbVbWR2w=s64","userId":"01451677567117313270"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model.fit(pred_train, tar_train)"],"execution_count":82,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.496e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.098e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.329e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.051e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=5.739e-01, previous alpha=5.739e-01, with an active set of 14 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.100e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.867e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.050e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.013e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=1.986e-01, previous alpha=1.960e-01, with an active set of 37 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.365e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.008e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.144e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.642e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.644e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.626e-01, previous alpha=5.354e-01, with an active set of 13 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.439e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.037e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=7.201e-01, previous alpha=7.200e-01, with an active set of 12 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=4.546e-02, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.539e-02, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 116 iterations, alpha=3.652e-02, previous alpha=3.599e-02, with an active set of 99 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.572e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.132e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.640e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.560e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.932e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.737e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=1.702e-01, previous alpha=1.687e-01, with an active set of 45 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.644e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.178e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.316e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.155e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.203e-01, previous alpha=3.135e-01, with an active set of 25 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.100e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.523e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.523e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 68 iterations, alpha=6.435e-02, previous alpha=6.382e-02, with an active set of 61 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.147e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.014e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.010e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.693e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.099e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.948e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.137e-01, previous alpha=3.116e-01, with an active set of 27 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.644e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.822e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=2.062e-01, previous alpha=1.984e-01, with an active set of 34 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.372e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.825e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.732e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.285e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.186e-01, previous alpha=5.185e-01, with an active set of 15 regressors.\n","  ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["LassoLarsCV(copy_X=True, cv=10, eps=2.220446049250313e-16, fit_intercept=True,\n","            max_iter=500, max_n_alphas=1000, n_jobs=None, normalize=True,\n","            positive=False, precompute=False, verbose=False)"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"5-6zc0nBQZJV","colab_type":"code","outputId":"75e40be9-6610-4524-f48d-dfb7f0ec4da9","executionInfo":{"status":"ok","timestamp":1574908304798,"user_tz":420,"elapsed":953,"user":{"displayName":"Madison Moye","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCbK9EQQQiDjMLG5iuvLnuGujaQR4mOw2TbVbWR2w=s64","userId":"01451677567117313270"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["#build coefficent chart\n","\n","predictors_model=pd.DataFrame(listofallpredictors)\n","predictors_model.columns = ['label']\n","predictors_model['coeff'] = model.coef_\n","\n","for index, row in predictors_model.iterrows():\n","    if row['coeff'] > 0:\n","        print(row.values)"],"execution_count":83,"outputs":[{"output_type":"stream","text":["['B01001014' 0.8558761066941788]\n","['B01001036' 2.5053482381631653]\n","['B01001037' 0.8892493223320962]\n","['B01001038' 1.5316387928880384]\n","['B02001005' 0.41252295298457853]\n","['B13014026' 0.48004105312075906]\n","['B13014027' 0.6978957445987839]\n","['B13016001' 875149895.329212]\n","['B19001017' 1.4834348068681533]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WXAKsg5VQ-DX","colab_type":"text"},"source":["**QUESTION #1:** In your own words, explain what the above lines of code are doing. Why am I doing it? Explain each line."]},{"cell_type":"markdown","metadata":{"id":"-Rep_fiVYwHP","colab_type":"text"},"source":["In terms of building the coefficient chart, the first line of code creates a table (predictors_model) that contains all predictors from the previous index, listofallpredictors, that we excluded the 7 repetitive variables. The second line of code names the columns 'label'. The third line of code adds a new column 'coeff' and all the coefficients from regression model. The last one is a for loop that \n","loops through the predictor model table and goes through the rows to find a row value that is larger than zero.  If so, it prints the value. \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SZzdsWckQ_NN","colab_type":"text"},"source":["**QUESTION #2:** Interpret each variable intuitively. What Census variables most predict sales? What does that mean, practically? Here's what I'd put for the example above. \"In areas where there are more females aged 30-34, we sell more Bobo Bars.\""]},{"cell_type":"markdown","metadata":{"id":"UnZxaSNRcpOL","colab_type":"text"},"source":["['B01001014' 0.8558761066941788] - In areas where there are more males aged 40-44, we sell 0.86 Bobo Bars.\n","\n","['B01001036' 2.5053482381631653] - In areas where there are more females aged 30-34, we sell 2.51 Bobo Bars.\n","\n","['B01001037' 0.8892493223320962] - In areas where there are more females aged 35-39, we sell 0.89 Bobo Bars.\n","\n","['B01001038' 1.5316387928880384] - In areas where there are more females aged 40-44, we sell 1.53 Bobo Bars.\n","\n","['B02001005' 0.41252295298457853] - In areas where there are more asians alone, we sell 0.41 Bobo Bars.\n","\n","['B13014026' 0.48004105312075906] - In areas where there are more females who gave birth in past 12 months aged 15-50 with Bachelor's degree, we sell 0.48 Bobo Bars.\n","\n","['B13014027' 0.6978957445987839] - In areas where there are more females who gave birth in past 12 months aged 15-50 with a Graduate or Professional degree, we sell 0.70 Bobo Bars.\n","\n","['B13016001' 875149895.329212] - In areas where there are more females who gave birth in past 12 months aged 15-50 , we sell 875,149,895.33 Bobo Bars.\n","\n","['B19001017' 1.4834348068681533] - In areas where household income is $200,000 or more, we sell 1.48 Bobo Bars."]},{"cell_type":"markdown","metadata":{"id":"ifTb6bOvRDX2","colab_type":"text"},"source":["**QUESTION #3:** If I had to report only two census variables to my boss that most steeply predicted sales, what would those be?"]},{"cell_type":"markdown","metadata":{"id":"ZrQyoSv1cxSH","colab_type":"text"},"source":["The most impressionable variable seems to be in ares where there are more females aged 15-50 who gave birth in the past 12 months, where we see more Bobo Bars. More specifically, 875,149,895 bars more. The second most impressionable variable was in areas where there are more Females aged 30 to 34 Years. More specifically, 2 more bars."]},{"cell_type":"markdown","metadata":{"id":"Bl08C0AnRGej","colab_type":"text"},"source":["Next, lets take a look at the mean squared error for the training and training set:"]},{"cell_type":"code","metadata":{"id":"e1v6CR9KRMTP","colab_type":"code","outputId":"91e506fb-c2a1-4bc9-b29f-c6ae9b196ee2","executionInfo":{"status":"ok","timestamp":1574908314011,"user_tz":420,"elapsed":1039,"user":{"displayName":"Madison Moye","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCbK9EQQQiDjMLG5iuvLnuGujaQR4mOw2TbVbWR2w=s64","userId":"01451677567117313270"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn.metrics import mean_squared_error\n","train_error = mean_squared_error(tar_train, model.predict(pred_train))\n","print ('training data MSE')\n","print(train_error)"],"execution_count":84,"outputs":[{"output_type":"stream","text":["training data MSE\n","22025.491066757\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PeY83YrqRdCy","colab_type":"text"},"source":["Run the code above, then do the same thing for your test sets (2) and (4) above. Compare the two outputs.\n"]},{"cell_type":"code","metadata":{"id":"EZqLb6PbRdee","colab_type":"code","outputId":"e47e79d2-470b-4b6a-9c2d-90deb1944928","executionInfo":{"status":"ok","timestamp":1574908316696,"user_tz":420,"elapsed":522,"user":{"displayName":"Madison Moye","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCbK9EQQQiDjMLG5iuvLnuGujaQR4mOw2TbVbWR2w=s64","userId":"01451677567117313270"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["test_error = mean_squared_error(tar_test, model.predict(pred_test))\n","print ('testing data MSE')\n","print(test_error)"],"execution_count":85,"outputs":[{"output_type":"stream","text":["testing data MSE\n","41549.54803776253\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X9K8z2mkScCX","colab_type":"text"},"source":["**QUESTION #4:** Are the training and text set mean squared errors similar? What does that mean practically?"]},{"cell_type":"markdown","metadata":{"id":"tNDFlscjSkA7","colab_type":"text"},"source":["Mean squared error: training data(22025.491066757) and testing data (41549.54803776253). These are not similar means. The training data's mean squared error is significantly smaller, which is the one we want considering that the smaller the value, the closer the fitted line is to the data points. The testing data mean squared error is just too big. "]},{"cell_type":"markdown","metadata":{"id":"hRR0rP_cShjW","colab_type":"text"},"source":["Next, let's see what our R-squared is for the training set: "]},{"cell_type":"code","metadata":{"id":"hTWc1mzFScrV","colab_type":"code","outputId":"e7d42ab6-2563-4fea-8d3e-87ebdd6d1d2e","executionInfo":{"status":"ok","timestamp":1574908319162,"user_tz":420,"elapsed":469,"user":{"displayName":"Madison Moye","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCbK9EQQQiDjMLG5iuvLnuGujaQR4mOw2TbVbWR2w=s64","userId":"01451677567117313270"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#r squared\n","rsquared_train=model.score(pred_train,tar_train)\n","print ('training data R-square')\n","print(rsquared_train)"],"execution_count":86,"outputs":[{"output_type":"stream","text":["training data R-square\n","0.2400221219784492\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rva4CM-WhLUl","colab_type":"code","outputId":"6881a98c-011f-47ad-9135-7a166b1453f6","executionInfo":{"status":"ok","timestamp":1574908321132,"user_tz":420,"elapsed":478,"user":{"displayName":"Madison Moye","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCbK9EQQQiDjMLG5iuvLnuGujaQR4mOw2TbVbWR2w=s64","userId":"01451677567117313270"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["rsquared_test=model.score(pred_test,tar_test)\n","print ('testing data R-square')\n","print(rsquared_test)"],"execution_count":87,"outputs":[{"output_type":"stream","text":["testing data R-square\n","0.1758628512005107\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zUYb9KmPzv1w","colab_type":"text"},"source":["Rsquared: training data(0.2400221219784492) and testing data (0.1758628512005107). For this one, you want a larger r squared value so the model fits better with your data. Therefore, we will go with the training data. "]},{"cell_type":"markdown","metadata":{"id":"lzBSsZbXSseB","colab_type":"text"},"source":["**QUESTION #5:** If your boss asked, \"How well does Census data, overall, predict sales?\" What would you say? Why?"]},{"cell_type":"markdown","metadata":{"id":"_IGMIdT1pqLP","colab_type":"text"},"source":["Based on the mean squared error and r squared, we would conclude that the census data is not a good fit to predict sales. The r-square value is still way too small from the training data to be beneficial in predict sales given that the range is from 0-100; it sits pretty low. In terms of the mean squared error, it is a little large. Overall, if we needed to use the census data, we would use the training data over the testing data but it would not be recommended to use this data for predicting sales. "]},{"cell_type":"markdown","metadata":{"id":"GVfcDMnmppLE","colab_type":"text"},"source":["Finally, let's see what our y-intercept is, so we can interpret what our baseline sales number looks like, all things considered:"]},{"cell_type":"code","metadata":{"id":"PAw3WsoHptYB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"53ec770a-9f37-4389-f73b-0bd407905729","executionInfo":{"status":"ok","timestamp":1574908324463,"user_tz":420,"elapsed":976,"user":{"displayName":"Madison Moye","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCbK9EQQQiDjMLG5iuvLnuGujaQR4mOw2TbVbWR2w=s64","userId":"01451677567117313270"}}},"source":["print(\"y interecept:\")\n","print(model.intercept_)"],"execution_count":88,"outputs":[{"output_type":"stream","text":["y interecept:\n","22.19738813257551\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q7l3HxFwp1R_","colab_type":"text"},"source":["**QUESTION #6:** What is our baseline sales number? What does that mean, practically? Think back to what y-intercepts mean in regression models."]},{"cell_type":"markdown","metadata":{"id":"xD_O2-7np3vH","colab_type":"text"},"source":["Our baseline sales number is 22.19738813257551 (or 22.20). The baseline sales number results in the number of Bobo bars sold, and representing the base which is 0. \n","\n"]},{"cell_type":"code","metadata":{"id":"FsCgAD7k3M3j","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}